{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c460d86",
   "metadata": {},
   "source": [
    "## A minimal example showing the code used train the standard VAE model in the deep learning project:\n",
    "Project 3: *Generative modelling for phenotypic profiling using Variational Autoencoders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed6d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import os\n",
    "from plotting import make_vae_plots\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import math \n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "from torch import nn, Tensor, sigmoid\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution, Bernoulli\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "from model_VAE_plus import PrintSize, Flatten, UnFlatten\n",
    "from model_VAE_plus import ReparameterizedDiagonalGaussian\n",
    "from model_VAE_plus import ReparameterizedDiagonalGaussianWithSigmoid\n",
    "from model_VAE_plus import VariationalAutoencoder, VariationalInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677f0a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      ">> Using device: cpu\n",
      "PyTorch Version 1.12.1\n",
      "Cuda Version None\n",
      "CUDNN Version None\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device(0))\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "print(\"PyTorch Version {}\" .format(torch.__version__))\n",
    "print(\"Cuda Version {}\" .format(torch.version.cuda))\n",
    "print(\"CUDNN Version {}\" .format(torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021df4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  7176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fa42510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'vae'\n",
    "result_dir = 'results_vanilla_vae_1/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "    \n",
    "# Set random seed for reproducibility\n",
    "#manualSeed = 999\n",
    "manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "f = open(result_dir + 'random_seed.txt', \"w\")\n",
    "f.write(str(manualSeed))\n",
    "f.close()\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adda73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evalutation and plotting functions\n",
    "def evaluation(test_loader, name=None, model_best=None, epoch=None,\n",
    "               device='cpu'):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "        model_best = model_best.to(device)\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, (test_batch, test_target) in enumerate(test_loader):\n",
    "        test_batch = test_batch.to(device)\n",
    "        \n",
    "        #loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss_t, xhat, diagnostics, outputs = vi(model_best, test_batch)\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 2\n",
    "    num_y = 3\n",
    "    x = next(iter(test_loader))[0].detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.transpose(x[i].reshape((3, 68, 68)), (1, 2, 0))\n",
    "        ax.imshow(plottable_image)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_real_images.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader))[0].detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.cpu()\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    \n",
    "    px = model_best.sample_from_prior(batch_size=num_x * num_y)['px']\n",
    "    x_samples = px.sample()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x_samples[i], (3, 68, 68)).permute(1, 2, 0)\n",
    "        ax.imshow(plottable_image)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_curve(name, nll_val, x_label=\"epochs\", y_label=\"nll\"):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.savefig(name + '.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3414c3",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4586106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "# Number of workers for dataloader\n",
    "workers = 1\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 6\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 68\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector\n",
    "latent_features = 100\n",
    "\n",
    "# Size of feature maps in VAE encoder and decoder\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Max patience for early stopping\n",
    "max_patience = 100\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr_VAE = 1e-4\n",
    "\n",
    "# Beta hyperparam for VAE loss\n",
    "beta = 1.0\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# The value the DMSO category is downsampled to\n",
    "downsample_value = 16000\n",
    "\n",
    "# Amount of data used for training, validation and testing\n",
    "data_prct = 1\n",
    "train_prct = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a984c8a",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset contain 68x68 images of single cells treated with different compounds. For each of the utilized compounds there is an associated mechanism of action (MOA), which describes how the compound it affecting the cell. There are 12 different MOA classes and a control class called DSMO. Here only a small percentage of the full dataset (1000 images) will be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc79143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.read_csv wiht pyarrow took 0.08883094787597656 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get current working directory\n",
    "DIR = os.getcwd()\n",
    "\n",
    "# Load metadata table\n",
    "start_time = time.time()\n",
    "metadata = pd.read_csv(\"../data/metadata_mini.csv\")\n",
    "print(\"pd.read_csv wiht pyarrow took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a4c0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moa</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Microtubule stabilizers</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DNA damage</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aurora kinase inhibitors</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DMSO</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eg5 inhibitors</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kinase inhibitors</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Epithelial</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Microtubule destabilizers</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Protein synthesis</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DNA replication</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Protein degradation</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cholesterol-lowering</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          moa  counts\n",
       "10    Microtubule stabilizers     387\n",
       "4                  DNA damage      81\n",
       "1    Aurora kinase inhibitors      79\n",
       "3                        DMSO      65\n",
       "6              Eg5 inhibitors      60\n",
       "8           Kinase inhibitors      60\n",
       "7                  Epithelial      59\n",
       "9   Microtubule destabilizers      54\n",
       "12          Protein synthesis      44\n",
       "0            Actin disruptors      38\n",
       "5             DNA replication      35\n",
       "11        Protein degradation      23\n",
       "2        Cholesterol-lowering      15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.groupby(\"moa\").size().reset_index(name='counts').sort_values(by=\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90be571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from class name to class index\n",
    "classes = {index: name for name, index in enumerate(metadata[\"moa\"].unique())}\n",
    "classes_inv = {v: k for k, v in classes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422b8cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Kinase inhibitors': 0,\n",
       " 'Eg5 inhibitors': 1,\n",
       " 'Microtubule stabilizers': 2,\n",
       " 'DMSO': 3,\n",
       " 'Microtubule destabilizers': 4,\n",
       " 'Protein synthesis': 5,\n",
       " 'Actin disruptors': 6,\n",
       " 'Epithelial': 7,\n",
       " 'Aurora kinase inhibitors': 8,\n",
       " 'DNA damage': 9,\n",
       " 'Protein degradation': 10,\n",
       " 'Cholesterol-lowering': 11,\n",
       " 'DNA replication': 12}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map from class name to class index\n",
    "classes = {index: name for name, index in enumerate(metadata[\"moa\"].unique())}\n",
    "classes_inv = {v: k for k, v in classes.items()}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "865abec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Multi_Cell_Image_Id</th>\n",
       "      <th>Multi_Cell_Image_Name</th>\n",
       "      <th>Single_Cell_Image_Id</th>\n",
       "      <th>Single_Cell_Image_Name</th>\n",
       "      <th>TableNumber</th>\n",
       "      <th>ImageNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>Image_FileName_Tubulin</th>\n",
       "      <th>Image_PathName_Tubulin</th>\n",
       "      <th>Image_FileName_Actin</th>\n",
       "      <th>Image_PathName_Actin</th>\n",
       "      <th>Image_Metadata_Plate_DAPI</th>\n",
       "      <th>Image_Metadata_Well_DAPI</th>\n",
       "      <th>Replicate</th>\n",
       "      <th>Image_Metadata_Compound</th>\n",
       "      <th>Image_Metadata_Concentration</th>\n",
       "      <th>moa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>191846</td>\n",
       "      <td>406091</td>\n",
       "      <td>406091</td>\n",
       "      <td>3080</td>\n",
       "      <td>Week7_7__F04_s2_w1C7A8F9CA-F54B-40DE-9EEE-7E71...</td>\n",
       "      <td>125</td>\n",
       "      <td>Week7_7__F04_s2_w1C7A8F9CA-F54B-40DE-9EEE-7E71...</td>\n",
       "      <td>7</td>\n",
       "      <td>3050</td>\n",
       "      <td>...</td>\n",
       "      <td>Week7_7__F04_s2_w2CAF44A0C-1EDB-41CE-8480-91F8...</td>\n",
       "      <td>Week7_34661</td>\n",
       "      <td>Week7_7__F04_s2_w4F9D05EDC-B012-4F3F-B558-5C56...</td>\n",
       "      <td>Week7_34661</td>\n",
       "      <td>Week7_34661</td>\n",
       "      <td>F04</td>\n",
       "      <td>2</td>\n",
       "      <td>PD-169316</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Kinase inhibitors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2787</td>\n",
       "      <td>5545</td>\n",
       "      <td>5545</td>\n",
       "      <td>525</td>\n",
       "      <td>Week10_200907_C09_s3_w19437640F-29D0-42B8-9C85...</td>\n",
       "      <td>83</td>\n",
       "      <td>Week10_200907_C09_s3_w19437640F-29D0-42B8-9C85...</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>Week10_200907_C09_s3_w29B6DE609-DB82-47FD-A103...</td>\n",
       "      <td>Week10_40111</td>\n",
       "      <td>Week10_200907_C09_s3_w44DE7F152-E698-48C6-87F2...</td>\n",
       "      <td>Week10_40111</td>\n",
       "      <td>Week10_40111</td>\n",
       "      <td>C09</td>\n",
       "      <td>1</td>\n",
       "      <td>AZ138</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Eg5 inhibitors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>159666</td>\n",
       "      <td>309483</td>\n",
       "      <td>309483</td>\n",
       "      <td>2387</td>\n",
       "      <td>Week5_130707_E02_s4_w1CD0139A5-C58F-4E4E-BE44-...</td>\n",
       "      <td>36</td>\n",
       "      <td>Week5_130707_E02_s4_w1CD0139A5-C58F-4E4E-BE44-...</td>\n",
       "      <td>5</td>\n",
       "      <td>3004</td>\n",
       "      <td>...</td>\n",
       "      <td>Week5_130707_E02_s4_w208EA0654-74FC-40AB-800F-...</td>\n",
       "      <td>Week5_29301</td>\n",
       "      <td>Week5_130707_E02_s4_w495146015-FBA6-406D-BD4F-...</td>\n",
       "      <td>Week5_29301</td>\n",
       "      <td>Week5_29301</td>\n",
       "      <td>E02</td>\n",
       "      <td>1</td>\n",
       "      <td>taxol</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Microtubule stabilizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>153731</td>\n",
       "      <td>292273</td>\n",
       "      <td>292273</td>\n",
       "      <td>2348</td>\n",
       "      <td>Week5_130707_D11_s2_w16C0AA106-F223-4CA3-976B-...</td>\n",
       "      <td>48</td>\n",
       "      <td>Week5_130707_D11_s2_w16C0AA106-F223-4CA3-976B-...</td>\n",
       "      <td>5</td>\n",
       "      <td>358</td>\n",
       "      <td>...</td>\n",
       "      <td>Week5_130707_D11_s2_w21FCEC64A-77E4-4257-8E5D-...</td>\n",
       "      <td>Week5_28921</td>\n",
       "      <td>Week5_130707_D11_s2_w46E781881-AAA6-4F65-9D1F-...</td>\n",
       "      <td>Week5_28921</td>\n",
       "      <td>Week5_28921</td>\n",
       "      <td>D11</td>\n",
       "      <td>2</td>\n",
       "      <td>taxol</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Microtubule stabilizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>169436</td>\n",
       "      <td>343661</td>\n",
       "      <td>343661</td>\n",
       "      <td>2634</td>\n",
       "      <td>Week6_200607_D02_s4_w1021438BB-B36F-48B2-98DE-...</td>\n",
       "      <td>26</td>\n",
       "      <td>Week6_200607_D02_s4_w1021438BB-B36F-48B2-98DE-...</td>\n",
       "      <td>6</td>\n",
       "      <td>564</td>\n",
       "      <td>...</td>\n",
       "      <td>Week6_200607_D02_s4_w2E5B36F5D-7732-4B5B-86B1-...</td>\n",
       "      <td>Week6_31681</td>\n",
       "      <td>Week6_200607_D02_s4_w47065B016-72C5-4F2B-BDE2-...</td>\n",
       "      <td>Week6_31681</td>\n",
       "      <td>Week6_31681</td>\n",
       "      <td>D02</td>\n",
       "      <td>3</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0.00</td>\n",
       "      <td>DMSO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>97113</td>\n",
       "      <td>183560</td>\n",
       "      <td>183560</td>\n",
       "      <td>2092</td>\n",
       "      <td>Week3_290607_F05_s2_w16BE8E365-50DA-470B-8D9B-...</td>\n",
       "      <td>150</td>\n",
       "      <td>Week3_290607_F05_s2_w16BE8E365-50DA-470B-8D9B-...</td>\n",
       "      <td>3</td>\n",
       "      <td>654</td>\n",
       "      <td>...</td>\n",
       "      <td>Week3_290607_F05_s2_w2185B8544-3831-4245-A00A-...</td>\n",
       "      <td>Week3_25461</td>\n",
       "      <td>Week3_290607_F05_s2_w47FBB3922-9DBE-450F-B484-...</td>\n",
       "      <td>Week3_25461</td>\n",
       "      <td>Week3_25461</td>\n",
       "      <td>F05</td>\n",
       "      <td>3</td>\n",
       "      <td>etoposide</td>\n",
       "      <td>3.00</td>\n",
       "      <td>DNA damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>171410</td>\n",
       "      <td>353345</td>\n",
       "      <td>353345</td>\n",
       "      <td>2593</td>\n",
       "      <td>Week6_200607_C11_s1_w10463464A-3744-4A1F-9083-...</td>\n",
       "      <td>38</td>\n",
       "      <td>Week6_200607_C11_s1_w10463464A-3744-4A1F-9083-...</td>\n",
       "      <td>6</td>\n",
       "      <td>2957</td>\n",
       "      <td>...</td>\n",
       "      <td>Week6_200607_C11_s1_w21D4F2DF2-F959-4696-B1DC-...</td>\n",
       "      <td>Week6_32121</td>\n",
       "      <td>Week6_200607_C11_s1_w4EBBCB968-9FAE-4424-97C4-...</td>\n",
       "      <td>Week6_32121</td>\n",
       "      <td>Week6_32121</td>\n",
       "      <td>C11</td>\n",
       "      <td>2</td>\n",
       "      <td>taxol</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Microtubule stabilizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>27835</td>\n",
       "      <td>54679</td>\n",
       "      <td>54679</td>\n",
       "      <td>745</td>\n",
       "      <td>Week1_150607_B11_s1_w129C9B1A2-75C6-44AE-9E1D-...</td>\n",
       "      <td>8</td>\n",
       "      <td>Week1_150607_B11_s1_w129C9B1A2-75C6-44AE-9E1D-...</td>\n",
       "      <td>1</td>\n",
       "      <td>277</td>\n",
       "      <td>...</td>\n",
       "      <td>Week1_150607_B11_s1_w284256C54-8558-4EDF-8C81-...</td>\n",
       "      <td>Week1_22141</td>\n",
       "      <td>Week1_150607_B11_s1_w49853504B-D04D-45BE-945A-...</td>\n",
       "      <td>Week1_22141</td>\n",
       "      <td>Week1_22141</td>\n",
       "      <td>B11</td>\n",
       "      <td>2</td>\n",
       "      <td>taxol</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Microtubule stabilizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>40876</td>\n",
       "      <td>75823</td>\n",
       "      <td>75823</td>\n",
       "      <td>736</td>\n",
       "      <td>Week1_150607_B06_s2_w12134F829-2C5E-4ED7-BA2B-...</td>\n",
       "      <td>149</td>\n",
       "      <td>Week1_150607_B06_s2_w12134F829-2C5E-4ED7-BA2B-...</td>\n",
       "      <td>1</td>\n",
       "      <td>2898</td>\n",
       "      <td>...</td>\n",
       "      <td>Week1_150607_B06_s2_w23FEAC940-8D8C-4735-A14E-...</td>\n",
       "      <td>Week1_22361</td>\n",
       "      <td>Week1_150607_B06_s2_w4899D2801-D3BC-45B8-8D35-...</td>\n",
       "      <td>Week1_22361</td>\n",
       "      <td>Week1_22361</td>\n",
       "      <td>B06</td>\n",
       "      <td>1</td>\n",
       "      <td>latrunculin B</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Actin disruptors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>211806</td>\n",
       "      <td>450231</td>\n",
       "      <td>450231</td>\n",
       "      <td>3336</td>\n",
       "      <td>Week8_4sites_E05_s4_w1F60943D4-F5BF-4C74-8131-...</td>\n",
       "      <td>4</td>\n",
       "      <td>Week8_4sites_E05_s4_w1F60943D4-F5BF-4C74-8131-...</td>\n",
       "      <td>8</td>\n",
       "      <td>2776</td>\n",
       "      <td>...</td>\n",
       "      <td>Week8_4sites_E05_s4_w2AF58FD78-3E72-4274-BFFD-...</td>\n",
       "      <td>Week8_38341</td>\n",
       "      <td>Week8_4sites_E05_s4_w44015C3C6-6223-4553-8321-...</td>\n",
       "      <td>Week8_38341</td>\n",
       "      <td>Week8_38341</td>\n",
       "      <td>E05</td>\n",
       "      <td>1</td>\n",
       "      <td>PP-2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Epithelial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  level_0   index  Unnamed: 0  Multi_Cell_Image_Id  \\\n",
       "0               0   191846  406091      406091                 3080   \n",
       "1               1     2787    5545        5545                  525   \n",
       "2               2   159666  309483      309483                 2387   \n",
       "3               3   153731  292273      292273                 2348   \n",
       "4               4   169436  343661      343661                 2634   \n",
       "..            ...      ...     ...         ...                  ...   \n",
       "995           995    97113  183560      183560                 2092   \n",
       "996           996   171410  353345      353345                 2593   \n",
       "997           997    27835   54679       54679                  745   \n",
       "998           998    40876   75823       75823                  736   \n",
       "999           999   211806  450231      450231                 3336   \n",
       "\n",
       "                                 Multi_Cell_Image_Name  Single_Cell_Image_Id  \\\n",
       "0    Week7_7__F04_s2_w1C7A8F9CA-F54B-40DE-9EEE-7E71...                   125   \n",
       "1    Week10_200907_C09_s3_w19437640F-29D0-42B8-9C85...                    83   \n",
       "2    Week5_130707_E02_s4_w1CD0139A5-C58F-4E4E-BE44-...                    36   \n",
       "3    Week5_130707_D11_s2_w16C0AA106-F223-4CA3-976B-...                    48   \n",
       "4    Week6_200607_D02_s4_w1021438BB-B36F-48B2-98DE-...                    26   \n",
       "..                                                 ...                   ...   \n",
       "995  Week3_290607_F05_s2_w16BE8E365-50DA-470B-8D9B-...                   150   \n",
       "996  Week6_200607_C11_s1_w10463464A-3744-4A1F-9083-...                    38   \n",
       "997  Week1_150607_B11_s1_w129C9B1A2-75C6-44AE-9E1D-...                     8   \n",
       "998  Week1_150607_B06_s2_w12134F829-2C5E-4ED7-BA2B-...                   149   \n",
       "999  Week8_4sites_E05_s4_w1F60943D4-F5BF-4C74-8131-...                     4   \n",
       "\n",
       "                                Single_Cell_Image_Name  TableNumber  \\\n",
       "0    Week7_7__F04_s2_w1C7A8F9CA-F54B-40DE-9EEE-7E71...            7   \n",
       "1    Week10_200907_C09_s3_w19437640F-29D0-42B8-9C85...            0   \n",
       "2    Week5_130707_E02_s4_w1CD0139A5-C58F-4E4E-BE44-...            5   \n",
       "3    Week5_130707_D11_s2_w16C0AA106-F223-4CA3-976B-...            5   \n",
       "4    Week6_200607_D02_s4_w1021438BB-B36F-48B2-98DE-...            6   \n",
       "..                                                 ...          ...   \n",
       "995  Week3_290607_F05_s2_w16BE8E365-50DA-470B-8D9B-...            3   \n",
       "996  Week6_200607_C11_s1_w10463464A-3744-4A1F-9083-...            6   \n",
       "997  Week1_150607_B11_s1_w129C9B1A2-75C6-44AE-9E1D-...            1   \n",
       "998  Week1_150607_B06_s2_w12134F829-2C5E-4ED7-BA2B-...            1   \n",
       "999  Week8_4sites_E05_s4_w1F60943D4-F5BF-4C74-8131-...            8   \n",
       "\n",
       "     ImageNumber  ...                             Image_FileName_Tubulin  \\\n",
       "0           3050  ...  Week7_7__F04_s2_w2CAF44A0C-1EDB-41CE-8480-91F8...   \n",
       "1             71  ...  Week10_200907_C09_s3_w29B6DE609-DB82-47FD-A103...   \n",
       "2           3004  ...  Week5_130707_E02_s4_w208EA0654-74FC-40AB-800F-...   \n",
       "3            358  ...  Week5_130707_D11_s2_w21FCEC64A-77E4-4257-8E5D-...   \n",
       "4            564  ...  Week6_200607_D02_s4_w2E5B36F5D-7732-4B5B-86B1-...   \n",
       "..           ...  ...                                                ...   \n",
       "995          654  ...  Week3_290607_F05_s2_w2185B8544-3831-4245-A00A-...   \n",
       "996         2957  ...  Week6_200607_C11_s1_w21D4F2DF2-F959-4696-B1DC-...   \n",
       "997          277  ...  Week1_150607_B11_s1_w284256C54-8558-4EDF-8C81-...   \n",
       "998         2898  ...  Week1_150607_B06_s2_w23FEAC940-8D8C-4735-A14E-...   \n",
       "999         2776  ...  Week8_4sites_E05_s4_w2AF58FD78-3E72-4274-BFFD-...   \n",
       "\n",
       "    Image_PathName_Tubulin                               Image_FileName_Actin  \\\n",
       "0              Week7_34661  Week7_7__F04_s2_w4F9D05EDC-B012-4F3F-B558-5C56...   \n",
       "1             Week10_40111  Week10_200907_C09_s3_w44DE7F152-E698-48C6-87F2...   \n",
       "2              Week5_29301  Week5_130707_E02_s4_w495146015-FBA6-406D-BD4F-...   \n",
       "3              Week5_28921  Week5_130707_D11_s2_w46E781881-AAA6-4F65-9D1F-...   \n",
       "4              Week6_31681  Week6_200607_D02_s4_w47065B016-72C5-4F2B-BDE2-...   \n",
       "..                     ...                                                ...   \n",
       "995            Week3_25461  Week3_290607_F05_s2_w47FBB3922-9DBE-450F-B484-...   \n",
       "996            Week6_32121  Week6_200607_C11_s1_w4EBBCB968-9FAE-4424-97C4-...   \n",
       "997            Week1_22141  Week1_150607_B11_s1_w49853504B-D04D-45BE-945A-...   \n",
       "998            Week1_22361  Week1_150607_B06_s2_w4899D2801-D3BC-45B8-8D35-...   \n",
       "999            Week8_38341  Week8_4sites_E05_s4_w44015C3C6-6223-4553-8321-...   \n",
       "\n",
       "    Image_PathName_Actin Image_Metadata_Plate_DAPI Image_Metadata_Well_DAPI  \\\n",
       "0            Week7_34661               Week7_34661                      F04   \n",
       "1           Week10_40111              Week10_40111                      C09   \n",
       "2            Week5_29301               Week5_29301                      E02   \n",
       "3            Week5_28921               Week5_28921                      D11   \n",
       "4            Week6_31681               Week6_31681                      D02   \n",
       "..                   ...                       ...                      ...   \n",
       "995          Week3_25461               Week3_25461                      F05   \n",
       "996          Week6_32121               Week6_32121                      C11   \n",
       "997          Week1_22141               Week1_22141                      B11   \n",
       "998          Week1_22361               Week1_22361                      B06   \n",
       "999          Week8_38341               Week8_38341                      E05   \n",
       "\n",
       "    Replicate Image_Metadata_Compound  Image_Metadata_Concentration  \\\n",
       "0           2               PD-169316                         10.00   \n",
       "1           1                   AZ138                          0.03   \n",
       "2           1                   taxol                          0.30   \n",
       "3           2                   taxol                          0.30   \n",
       "4           3                    DMSO                          0.00   \n",
       "..        ...                     ...                           ...   \n",
       "995         3               etoposide                          3.00   \n",
       "996         2                   taxol                          0.30   \n",
       "997         2                   taxol                          0.30   \n",
       "998         1           latrunculin B                          1.00   \n",
       "999         1                    PP-2                          3.00   \n",
       "\n",
       "                         moa  \n",
       "0          Kinase inhibitors  \n",
       "1             Eg5 inhibitors  \n",
       "2    Microtubule stabilizers  \n",
       "3    Microtubule stabilizers  \n",
       "4                       DMSO  \n",
       "..                       ...  \n",
       "995               DNA damage  \n",
       "996  Microtubule stabilizers  \n",
       "997  Microtubule stabilizers  \n",
       "998         Actin disruptors  \n",
       "999               Epithelial  \n",
       "\n",
       "[1000 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4601d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader class. Using the metadata table, images are sampled and \n",
    "# passed to VAE duing training\n",
    "class SingleCellDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, images_folder, class_map, \n",
    "                 mode='train', transform = None):\n",
    "        self.df = annotation_file\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.class2index = class_map\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.loc[index, \"Single_Cell_Image_Name\"]\n",
    "        label = self.class2index[self.df.loc[index, \"moa\"]]\n",
    "        #subfolder = re.search(\"(.*)_\", filename).group(1)\n",
    "        image = np.load(os.path.join(self.images_folder, filename))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image.astype(np.float32))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a4aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n",
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# The loaders perform the actual work\n",
    "#images_folder = '/zhome/70/5/14854/nobackup/deeplearningf22/bbbc021/singlecell/singh_cp_pipeline_singlecell_images/'\n",
    "images_folder = \"../data/\"\n",
    "train_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x/x.max()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = SingleCellDataset(images_folder=images_folder, \n",
    "                              annotation_file=metadata, \n",
    "                              transform=train_transforms,\n",
    "                              class_map=classes)\n",
    "\n",
    "# Define the size of the train, validation and test datasets\n",
    "data_amount = int(len(metadata) * data_prct)\n",
    "train_size = int(train_prct * data_amount)\n",
    "val_size = (data_amount - train_size) // 2\n",
    "test_size = (data_amount - train_size) // 2\n",
    "\n",
    "indicies = torch.randperm(len(metadata))\n",
    "train_indices = indicies[:train_size]\n",
    "val_indicies = indicies[train_size:train_size+val_size]\n",
    "test_indicies = indicies[train_size+val_size:train_size+val_size+test_size]\n",
    "\n",
    "training_set = torch.utils.data.Subset(train_set, train_indices.tolist())\n",
    "val_set = torch.utils.data.Subset(train_set, val_indicies.tolist())\n",
    "testing_set = torch.utils.data.Subset(train_set, test_indicies.tolist())\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, \n",
    "                                             shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testing_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(training_loader.dataset))\n",
    "print(len(val_loader.dataset))\n",
    "print(len(test_loader.dataset))\n",
    "\n",
    "# Load a batch of images into memory\n",
    "images, labels = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea1dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(6, 6), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): PrintSize()\n",
      "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): PrintSize()\n",
      "    (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): PrintSize()\n",
      "    (12): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): PrintSize()\n",
      "    (16): Conv2d(512, 200, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (17): PrintSize()\n",
      "    (18): Flatten()\n",
      "    (19): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (20): PrintSize()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): UnFlatten()\n",
      "    (1): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): PrintSize()\n",
      "    (5): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): PrintSize()\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): PrintSize()\n",
      "    (13): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): PrintSize()\n",
      "    (17): ConvTranspose2d(64, 6, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (18): PrintSize()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoencoder(latent_features)\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84692abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "897f847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([6, 64, 32, 32])\n",
      "Size: torch.Size([6, 128, 16, 16])\n",
      "Size: torch.Size([6, 256, 8, 8])\n",
      "Size: torch.Size([6, 512, 4, 4])\n",
      "Size: torch.Size([6, 200, 1, 1])\n",
      "Size: torch.Size([6, 200])\n",
      "Size: torch.Size([6, 512, 4, 4])\n",
      "Size: torch.Size([6, 256, 8, 8])\n",
      "Size: torch.Size([6, 128, 16, 16])\n",
      "Size: torch.Size([6, 64, 32, 32])\n",
      "Size: torch.Size([6, 6, 68, 68])\n",
      "xhat   | shape: [6, 3, 68, 68]\n",
      "loss   | mean =   2347.119, shape: []\n",
      "elbo   | mean =  -2347.119, shape: [6]\n",
      "log_px | mean =  -2335.891, shape: [6]\n",
      "kl     | mean =     11.228, shape: [6]\n"
     ]
    }
   ],
   "source": [
    "vi_test = VariationalInference(beta=1)\n",
    "loss, xhat, diagnostics, outputs = vi_test(vae, images)\n",
    "print(f\"{'xhat':6} | shape: {list(xhat.shape)}\")\n",
    "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
    "for key, tensor in diagnostics.items():\n",
    "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74877272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "name = 'vae'\n",
    "\n",
    "from collections import defaultdict\n",
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# VAE\n",
    "vae = VariationalAutoencoder(latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr_VAE)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "epoch = 0\n",
    "nll_val = []\n",
    "elbo_loss = []\n",
    "kl_loss = []\n",
    "log_px_loss = []\n",
    "train_loss = []\n",
    "best_nll = 1000000.\n",
    "patience = 0\n",
    "step = 0\n",
    "train_img_list = []\n",
    "val_img_list = []\n",
    "\n",
    "train_fixed, train_label_fixed = next(iter(val_loader))\n",
    "train_fixed_b1, train_fixed_b2 = torch.split(train_fixed, split_size_or_sections=batch_size//2)\n",
    "train_fixed_b1 = train_fixed_b1.reshape(batch_size//2, 3, 68, 68)\n",
    "train_fixed_b1 = train_fixed_b1.to(device)\n",
    "\n",
    "val_fixed, val_label_fixed = next(iter(val_loader))\n",
    "val_fixed_b1, val_fixed_b2 = torch.split(val_fixed, split_size_or_sections=batch_size//2)\n",
    "val_fixed_b1 = val_fixed_b1.reshape(batch_size//2, 3, 68, 68)\n",
    "val_fixed_b1 = val_fixed_b1.to(device)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6886df",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f895484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 10\n",
      "Size: torch.Size([6, 64, 32, 32])\n",
      "Size: torch.Size([6, 128, 16, 16])\n",
      "Size: torch.Size([6, 256, 8, 8])\n",
      "Size: torch.Size([6, 512, 4, 4])\n",
      "Size: torch.Size([6, 200, 1, 1])\n",
      "Size: torch.Size([6, 200])\n",
      "Size: torch.Size([6, 512, 4, 4])\n",
      "Size: torch.Size([6, 256, 8, 8])\n",
      "Size: torch.Size([6, 128, 16, 16])\n",
      "Size: torch.Size([6, 64, 32, 32])\n",
      "Size: torch.Size([6, 6, 68, 68])\n",
      "[1/10][49/158]\tLoss_VAE: 683.5329\n",
      "[1/10][99/158]\tLoss_VAE: 357.1469\n",
      "[1/10][149/158]\tLoss_VAE: 369.3882\n",
      "saved!\n",
      "2 / 10\n",
      "[2/10][41/158]\tLoss_VAE: 310.6158\n",
      "[2/10][91/158]\tLoss_VAE: 277.9490\n",
      "[2/10][141/158]\tLoss_VAE: 290.6258\n",
      "saved!\n",
      "3 / 10\n",
      "[3/10][33/158]\tLoss_VAE: 300.3620\n",
      "[3/10][83/158]\tLoss_VAE: 246.5753\n",
      "[3/10][133/158]\tLoss_VAE: 244.5241\n",
      "saved!\n",
      "4 / 10\n",
      "[4/10][25/158]\tLoss_VAE: 340.7279\n",
      "[4/10][75/158]\tLoss_VAE: 223.7568\n",
      "[4/10][125/158]\tLoss_VAE: 277.3119\n",
      "saved!\n",
      "5 / 10\n",
      "[5/10][17/158]\tLoss_VAE: 297.5498\n",
      "[5/10][67/158]\tLoss_VAE: 377.0596\n",
      "[5/10][117/158]\tLoss_VAE: 327.6242\n",
      "6 / 10\n",
      "[6/10][9/158]\tLoss_VAE: 263.2485\n",
      "[6/10][59/158]\tLoss_VAE: 276.4604\n",
      "[6/10][109/158]\tLoss_VAE: 282.9721\n",
      "7 / 10\n",
      "[7/10][1/158]\tLoss_VAE: 228.1979\n",
      "[7/10][51/158]\tLoss_VAE: 216.3184\n",
      "[7/10][101/158]\tLoss_VAE: 168.2967\n",
      "[7/10][151/158]\tLoss_VAE: 188.3123\n",
      "8 / 10\n",
      "[8/10][43/158]\tLoss_VAE: 194.8660\n",
      "[8/10][93/158]\tLoss_VAE: 159.8611\n",
      "[8/10][143/158]\tLoss_VAE: 169.7169\n",
      "saved!\n",
      "9 / 10\n",
      "[9/10][35/158]\tLoss_VAE: 242.5001\n",
      "[9/10][85/158]\tLoss_VAE: 181.6436\n",
      "[9/10][135/158]\tLoss_VAE: 210.0838\n",
      "10 / 10\n",
      "[10/10][27/158]\tLoss_VAE: 347.6790\n",
      "[10/10][77/158]\tLoss_VAE: 199.6508\n",
      "[10/10][127/158]\tLoss_VAE: 249.7582\n"
     ]
    }
   ],
   "source": [
    "while epoch < num_epochs:\n",
    "    epoch += 1\n",
    "    print(f\"{epoch} / {num_epochs}\")\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for i, (x, y) in enumerate(training_loader):\n",
    "        step += 1\n",
    "        \n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, xhat, diagnostics, outputs = vi(vae, x)\n",
    "\n",
    "        elbo_loss.append(diagnostics['elbo'].mean().item())\n",
    "        kl_loss.append(diagnostics['kl'].mean().item())\n",
    "        log_px_loss.append(diagnostics['log_px'].mean().item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "        \n",
    "        # Output training stats\n",
    "        if step % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_VAE: %.4f'\n",
    "                % (epoch, num_epochs, i, len(training_loader), loss.item()))\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (step % 100 == 0) or ((epoch == num_epochs-1) and (i == len(training_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                loss_elbo, xhat, diagnostics, outputs = vi(vae, train_fixed_b1)\n",
    "                xhat = xhat.detach().cpu()\n",
    "            train_img_list.append(vutils.make_grid(xhat, padding=2, normalize=True))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss_elbo, xhat, diagnostics, outputs = vi(vae, val_fixed_b1)\n",
    "                xhat = xhat.detach().cpu()\n",
    "            val_img_list.append(vutils.make_grid(xhat, padding=2, normalize=True))\n",
    "            \n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "        \n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        \n",
    "        # Just load a single batch from the validation loader\n",
    "        x, y = next(iter(val_loader))\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss_val, xhat, diagnostics, outputs = vi(vae, x)\n",
    "        nll_val.append(loss_val.cpu())  # save for plotting\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "    \n",
    "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
    "    if (step % 3000 == 0):\n",
    "        filename = result_dir + f\"vae_out_{epoch}.png\"\n",
    "        make_vae_plots(vae, x, y, outputs, training_data, validation_data, save_img = filename, save=True)\n",
    "\n",
    "    if epoch == 1:\n",
    "            print('saved!')\n",
    "            torch.save(vae, result_dir + name + '.model')\n",
    "            best_nll = loss_val\n",
    "    else:\n",
    "        samples_generated(result_dir + name, val_loader, extra_name=\"_epoch_\" + str(epoch))\n",
    "        if loss_val < best_nll:\n",
    "            print('saved!')\n",
    "            torch.save(vae, result_dir + name + '.model')\n",
    "            best_nll = loss_val\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience = patience + 1\n",
    "        \n",
    "    if patience > max_patience:\n",
    "        print(\"Max patience reached! Performing early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9bb6e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved final model!\n",
      "Could not generate the plot of the latent sanples because of exception\n",
      "name 'sns' is not defined\n"
     ]
    }
   ],
   "source": [
    "print('saved final model!')\n",
    "torch.save(vae, result_dir + name + '_final.model')\n",
    "\n",
    "make_vae_plots(vae, x, y, outputs, training_data, validation_data,\n",
    "               save_img = result_dir + \"vae_out_final.png\", save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994315c",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d634164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS: nll=40.99229125976562\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader, device=device)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_real(result_dir + name, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a31940b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(result_dir + name, nll_val)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Elbo Loss During Training\")\n",
    "plt.plot(elbo_loss, label=\"Elbo\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(result_dir + 'elbo_training_loss.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"KL Loss During Training\")\n",
    "plt.plot(elbo_loss, label=\"KL\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(result_dir + 'kl_training_loss.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Log(p(x|z)) Loss During Training\")\n",
    "plt.plot(elbo_loss, label=\"Log(p(x|z))\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(result_dir + 'log_px_training_loss.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#fig = plt.figure(figsize=(8,8))\n",
    "#plt.axis(\"off\")\n",
    "#ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in train_img_list]\n",
    "#ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "## saving to m4 using ffmpeg writer\n",
    "#writervideo = animation.FFMpegWriter(fps=60)\n",
    "#ani.save(result_dir + 'train_VAE_progression.mp4', writer=writervideo)\n",
    "#plt.close()\n",
    "\n",
    "#fig = plt.figure(figsize=(8,8))\n",
    "#plt.axis(\"off\")\n",
    "#ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in val_img_list]\n",
    "#ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "## saving to m4 using ffmpeg writer\n",
    "#writervideo = animation.FFMpegWriter(fps=60)\n",
    "#ani.save(result_dir + 'val_VAE_progression.mp4', writer=writervideo)\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf39bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
