{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d274252",
   "metadata": {},
   "source": [
    "## A minimal example showing the code used train classifier models using VAE/VAE+ latent represenationa of single-cell images that was used in the deep learning project:\n",
    "Project 3: *Generative modelling for phenotypic profiling using Variational Autoencoders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e8728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn, Tensor, sigmoid\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn import metrics\n",
    "import math \n",
    "from typing import *\n",
    "from collections import defaultdict\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution, Bernoulli\n",
    "import torchvision.utils as vutils\n",
    "from torch.nn.functional import softplus\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "from sklearn import metrics\n",
    "from model_VAE_plus import SingleCellDataset, PrintSize, Flatten, UnFlatten\n",
    "from model_VAE_plus import ReparameterizedDiagonalGaussian\n",
    "from model_VAE_plus import ReparameterizedDiagonalGaussianWithSigmoid\n",
    "from model_VAE_plus import VariationalAutoencoder, VariationalInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea48140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  7081\n",
      ">> Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "name = 'classifier_plus_VAE'\n",
    "result_dir = 'classifier_results_plus_VAE/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "#manualSeed = 999\n",
    "manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "f = open(result_dir + 'random_seed.txt', \"w\")\n",
    "f.write(str(manualSeed))\n",
    "f.close()\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb9a67",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c56f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model to be used\n",
    "vae_model = 'results_plus_VAE/vae_plus_final.model'\n",
    "\n",
    "# Initialize parameters\n",
    "# Number of workers for dataloader\n",
    "workers = 1\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 6\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 68\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector\n",
    "latent_features = 100\n",
    "\n",
    "# Size of feature maps in VAE encoder and decoder\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Validation frequency\n",
    "validation_every_steps = 100\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Max patience for early stopping\n",
    "max_patience = 30\n",
    "\n",
    "# Learning rate for optimizer\n",
    "lr = 3e-4\n",
    "\n",
    "# Beta hyperparam for VAE loss\n",
    "beta = 1.0\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# The value the DMSO category is downsampled to\n",
    "downsample_value = 16000\n",
    "\n",
    "# Amount of data used for training, validation and testing\n",
    "data_prct = 1\n",
    "train_prct = 0.85\n",
    "\n",
    "# Number of classes\n",
    "n_classes = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c66526",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset contain 68x68 images of single cells treated with different compounds. For each of the utilized compounds there is an associated mechanism of action (MOA), which describes how the compound it affecting the cell. There are 12 different MOA classes and a control class called DSMO. Here only a small percentage of the full dataset (1000 images) will be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813e81cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.read_csv wiht pyarrow took 0.052648067474365234 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load metadata table\n",
    "start_time = time.time()\n",
    "metadata = pd.read_csv(\"../data/metadata_mini.csv\")\n",
    "print(\"pd.read_csv wiht pyarrow took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1278196b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moa</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Microtubule stabilizers</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DNA damage</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aurora kinase inhibitors</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DMSO</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eg5 inhibitors</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kinase inhibitors</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Epithelial</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Microtubule destabilizers</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Protein synthesis</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DNA replication</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Protein degradation</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cholesterol-lowering</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          moa  counts\n",
       "10    Microtubule stabilizers     387\n",
       "4                  DNA damage      81\n",
       "1    Aurora kinase inhibitors      79\n",
       "3                        DMSO      65\n",
       "6              Eg5 inhibitors      60\n",
       "8           Kinase inhibitors      60\n",
       "7                  Epithelial      59\n",
       "9   Microtubule destabilizers      54\n",
       "12          Protein synthesis      44\n",
       "0            Actin disruptors      38\n",
       "5             DNA replication      35\n",
       "11        Protein degradation      23\n",
       "2        Cholesterol-lowering      15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.groupby(\"moa\").size().reset_index(name='counts').sort_values(by=\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4480843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from class name to class index\n",
    "classes = {index: name for name, index in enumerate(metadata[\"moa\"].unique())}\n",
    "classes_inv = {v: k for k, v in classes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe01345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader class. Using the metadata table, images are sampled and \n",
    "# passed to VAE duing training\n",
    "class SingleCellDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, images_folder, class_map, \n",
    "                 mode='train', transform = None):\n",
    "        self.df = annotation_file\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.class2index = class_map\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.loc[index, \"Single_Cell_Image_Name\"]\n",
    "        label = self.class2index[self.df.loc[index, \"moa\"]]\n",
    "        #subfolder = re.search(\"(.*)_\", filename).group(1)\n",
    "        image = np.load(os.path.join(self.images_folder, filename))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image.astype(np.float32))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "215395d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "75\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# The loaders perform the actual work\n",
    "#images_folder = '/zhome/70/5/14854/nobackup/deeplearningf22/bbbc021/singlecell/singh_cp_pipeline_singlecell_images/'\n",
    "images_folder = \"../data/\"\n",
    "train_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x/x.max()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = SingleCellDataset(images_folder=images_folder, \n",
    "                              annotation_file=metadata, \n",
    "                              transform=train_transforms,\n",
    "                              class_map=classes)\n",
    "\n",
    "# Define the size of the train, validation and test datasets\n",
    "data_amount = int(len(metadata) * data_prct)\n",
    "train_size = int(train_prct * data_amount)\n",
    "val_size = (data_amount - train_size) // 2\n",
    "test_size = (data_amount - train_size) // 2\n",
    "\n",
    "indicies = torch.randperm(len(metadata))\n",
    "train_indices = indicies[:train_size]\n",
    "val_indicies = indicies[train_size:train_size+val_size]\n",
    "test_indicies = indicies[train_size+val_size:train_size+val_size+test_size]\n",
    "\n",
    "training_set = torch.utils.data.Subset(train_set, train_indices.tolist())\n",
    "val_set = torch.utils.data.Subset(train_set, val_indicies.tolist())\n",
    "testing_set = torch.utils.data.Subset(train_set, test_indicies.tolist())\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, \n",
    "                                             shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testing_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(training_loader.dataset))\n",
    "print(len(val_loader.dataset))\n",
    "print(len(test_loader.dataset))\n",
    "\n",
    "# Load a batch of images into memory\n",
    "images, labels = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26a164b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(6, 6), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): PrintSize()\n",
      "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): PrintSize()\n",
      "    (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): PrintSize()\n",
      "    (12): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): PrintSize()\n",
      "    (16): Conv2d(512, 200, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (17): PrintSize()\n",
      "    (18): Flatten()\n",
      "    (19): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (20): PrintSize()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): UnFlatten()\n",
      "    (1): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): PrintSize()\n",
      "    (5): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): PrintSize()\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): PrintSize()\n",
      "    (13): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): PrintSize()\n",
      "    (17): ConvTranspose2d(64, 6, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (18): PrintSize()\n",
      "  )\n",
      ")\n",
      "torch.Size([6, 3, 68, 68])\n",
      "Size: torch.Size([6, 64, 32, 32])\n",
      "Size: torch.Size([6, 128, 16, 16])\n",
      "Size: torch.Size([6, 256, 8, 8])\n",
      "Size: torch.Size([6, 512, 4, 4])\n",
      "Size: torch.Size([6, 200, 1, 1])\n",
      "Size: torch.Size([6, 200])\n",
      "Size: torch.Size([6, 512, 4, 4])\n",
      "Size: torch.Size([6, 256, 8, 8])\n",
      "Size: torch.Size([6, 128, 16, 16])\n",
      "Size: torch.Size([6, 64, 32, 32])\n",
      "Size: torch.Size([6, 6, 68, 68])\n",
      "loss   | mean =   2449.349, shape: []\n",
      "elbo   | mean =  -2449.349, shape: [6]\n",
      "log_px | mean =  -2440.559, shape: [6]\n",
      "kl     | mean =      8.789, shape: [6]\n"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoencoder(latent_features)\n",
    "loss_fn = nn.MSELoss(reduction='none')\n",
    "print(vae)\n",
    "\n",
    "# Test with random input\n",
    "vi_test = VariationalInference(beta=1)\n",
    "print(images.shape)\n",
    "loss, xhat, diagnostics, outputs = vi_test(vae, images)\n",
    "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
    "for key, tensor in diagnostics.items():\n",
    "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ccec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_features, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        activation_fn = nn.ReLU\n",
    "        \n",
    "        \"\"\" self.net = nn.Sequential(\n",
    "          \n",
    "            nn.Linear(latent_features, 512),\n",
    "            activation_fn(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 1024),\n",
    "            activation_fn(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            activation_fn(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            activation_fn(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 100),\n",
    "            activation_fn(),\n",
    "            nn.Linear(100, n_classes)\n",
    "            #nn.Flatten()\n",
    "        ) \"\"\"\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_features, 512),\n",
    "            activation_fn(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 1024),\n",
    "            activation_fn(),\n",
    "            nn.Linear(1024, 300),\n",
    "            activation_fn(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(300, 100),\n",
    "            activation_fn(),\n",
    "            nn.Linear(100, n_classes)\n",
    "            #nn.Flatten()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d556c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (net): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=100, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=1024, out_features=300, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=300, out_features=100, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=100, out_features=13, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifierNet = Classifier(latent_features, n_classes)\n",
    "print(classifierNet)\n",
    "classifierNet = classifierNet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(classifierNet.parameters(), lr=lr)  \n",
    "\n",
    "latentNet = torch.load(vae_model)\n",
    "latentNet = latentNet.to(device)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "vi = VariationalInference(beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26ad8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def compute_confusion_matrix(target, pred, normalize=None):\n",
    "    return metrics.confusion_matrix(\n",
    "        target.detach().cpu().numpy(), \n",
    "        pred.detach().cpu().numpy(),\n",
    "        normalize=normalize\n",
    "    )\n",
    "\n",
    "def normalize(matrix, axis):\n",
    "    axis = {'true': 1, 'pred': 0}[axis]\n",
    "    return matrix / matrix.sum(axis=axis, keepdims=True)\n",
    "\n",
    "step = 0\n",
    "classifierNet.train()\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_acc, test_loss = [], []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "loss_val = 1000000-1\n",
    "best_nll = 1000000\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf3b54",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b57f3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n",
      "Finished training.\n",
      "saved final model!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    train_loss_batches = []\n",
    "    \n",
    "    for inputs, targets in training_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass, compute gradients, perform one training step.\n",
    "        loss, xhat, diagnostics, vae_outputs = vi(latentNet, inputs)\n",
    "        z = vae_outputs['z']\n",
    "        z = z.to(device)\n",
    "\n",
    "        # Forward pass.\n",
    "        output = classifierNet(z)\n",
    "        output = output.to(device)\n",
    "        output = output.reshape(-1, 13)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = criterion(output, targets)\n",
    "        train_loss_batches.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Increment step counter\n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            train_loss.append(np.mean(train_loss_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "            train_loss_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            valid_accuracies_batches = []\n",
    "            valid_loss_batches = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                classifierNet.eval()\n",
    "                for inputs, targets in test_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    loss_vae_val, xhat, diagnostics, vae_outputs = vi(latentNet, inputs)\n",
    "                    z = vae_outputs['z']\n",
    "                    z = z.to(device)\n",
    "\n",
    "                    output = classifierNet(z)\n",
    "                    output = output.reshape(-1,13)\n",
    "\n",
    "                    loss_val = criterion(output, targets)\n",
    "                    valid_loss_batches.append(loss_val.detach().cpu().numpy())\n",
    "\n",
    "                    predictions = output.max(1)[1]\n",
    "\n",
    "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "                classifierNet.train()\n",
    "                \n",
    "            # Append average validation accuracy to list.\n",
    "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(testing_set))\n",
    "            val_loss.append(np.sum(valid_loss_batches) / len(testing_set))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "            print(f\"                 validation accuracy: {valid_accuracies[-1]}\")\n",
    "\n",
    "\n",
    "    if loss_val < best_nll:\n",
    "        print('saved!')\n",
    "        torch.save(classifierNet, result_dir + name + '.model')\n",
    "        best_nll = loss_val\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience = patience + 1\n",
    "            \n",
    "    if patience > max_patience:\n",
    "        print(\"Max patience reached! Performing early stopping!\")\n",
    "        break\n",
    "\n",
    "print(\"Finished training.\")\n",
    "print('saved final model!')\n",
    "torch.save(classifierNet, result_dir + name + '_final.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b4fa7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a187747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test set\n",
    "test_true, test_pred = np.array([]), np.array([])\n",
    "confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "best_classifierNet = torch.load(result_dir + name + '.model')\n",
    "best_classifierNet = best_classifierNet.to(device)\n",
    "with torch.no_grad():\n",
    "    classifierNet.eval()\n",
    "    test_accuracies = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        loss_vae_test, xhat, diagnostics, vae_outputs = vi(latentNet, inputs)\n",
    "        z = vae_outputs['z']\n",
    "        z = z.to(device)\n",
    "\n",
    "        output = best_classifierNet(z)\n",
    "        output = output.reshape(-1,13)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        predictions = output.max(1)[1]\n",
    "\n",
    "        # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).\n",
    "        test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n",
    "        test_true = np.append(test_true, targets.detach().cpu().numpy())\n",
    "        test_pred = np.append(test_pred, predictions.detach().cpu().numpy())\n",
    "\n",
    "    confusion_matrix = metrics.confusion_matrix(\n",
    "        test_true, \n",
    "        test_pred,\n",
    "        normalize=None)\n",
    "\n",
    "    test_accuracy = np.sum(test_accuracies) / len(testing_set)\n",
    "    \n",
    "    classifierNet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9fec3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.387\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "f = open(result_dir + name + '_test_accuracy.txt', \"w\")\n",
    "f.write(str(test_accuracy))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee667cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = [classes[i] for i in classes]\n",
    "y_labels = x_labels\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(\n",
    "    ax=plt.gca(),\n",
    "    data=normalize(confusion_matrix, 'true'),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cmap=\"Reds\",\n",
    "    cbar=False,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=x_labels,\n",
    "    yticklabels=y_labels,\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.ylabel(\"True class\")\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(result_dir + 'confusion_matrix.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=x_labels, y=np.diag(normalize(confusion_matrix, 'true')))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Per-class accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(result_dir + 'per_class_accuracy.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "plt.figure()\n",
    "#plt.plot(range(50,step, 50), train_accuracies, 'r', range(50,step, 50), valid_accuracies, 'b')\n",
    "plt.plot(train_accuracies, label=\"Train Accucary\")\n",
    "plt.legend()\n",
    "plt.xlabel('steps'), plt.ylabel('Acc')\n",
    "plt.savefig(result_dir + 'classification_train_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "#plt.plot(range(50,step, 50), train_accuracies, 'r', range(50,step, 50), valid_accuracies, 'b')\n",
    "plt.plot(valid_accuracies, label=\"Validation Accucary\")\n",
    "plt.legend()\n",
    "plt.xlabel('steps'), plt.ylabel('Acc')\n",
    "plt.savefig(result_dir + 'classification_val_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training loss during training\")\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(result_dir + 'training_loss.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Validation loss during training\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(result_dir + 'validation_loss.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "np.save(result_dir + 'train_accuracies.npy', train_accuracies)\n",
    "np.save(result_dir +'valid_accuracies.npy',  valid_accuracies)\n",
    "np.save(result_dir + 'train_loss.npy', train_loss)\n",
    "np.save(result_dir +'val_loss.npy',  val_loss)\n",
    "torch.save(classifierNet, result_dir + name + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae11ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
